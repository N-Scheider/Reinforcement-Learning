{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1db3e6c3-1008-4cb9-b13b-1a1295b76e9b",
   "metadata": {},
   "source": [
    "#  Mathematics of Reinforcement Learning: Homework Sheet 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a401c27c",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "The theoretical exercise 1 can be found in the PDF file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89e99cb",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "In this exercise we use the Bellman optimality equation to compute the optimal policy in the optimal investment problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f891cb51-d3be-4a81-ac6f-087af1a88555",
   "metadata": {},
   "source": [
    "Make sure you have all the necessary packages (numpy, matplotlib) installed. You can use `conda install <package>` or `pip install <package>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c45876d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c234faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We define the start state, which will be used in some test functions.\n",
    "start_state = (0., 8., 100.)\n",
    "\n",
    "#market parameters\n",
    "T = 3     #Time horizon\n",
    "u = 2.0   #CRR parameter, up factor change, slightly different from the other exercises!\n",
    "d = -.5   #CRR parameter, down factor change\n",
    "q = .5    #CRR parameter, probability of up change"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c43fa71",
   "metadata": {},
   "source": [
    "**Task 1:** Implement a function `get_admissible_actions(p, w)` that takes as input float values `p` and `w` representing the current asset price `p` and wealth `w` and returns a list of all possible action values in the form of float values.\n",
    "\n",
    "*Reminder:* It is only possible to buy integer amounts of the financial asset. The amount must be non-negative and can't exceed the current wealth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7ada537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_admissible_actions(p, w):\n",
    "    # ---- TODO ----\n",
    "    action_values = []\n",
    "    \n",
    "    # floor(w/p) maximum amount of purchasable asset units\n",
    "    # from 0 to that value * the asset price, this gives back investment sums possible\n",
    "    \n",
    "    asset_units = list(range(int(np.floor(w/p)+1)))\n",
    "    asset_investment = [(p*i) for i in asset_units]\n",
    "    \n",
    "    return asset_investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dd57ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert get_admissible_actions(10,20) == [0,10,20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bd4e56",
   "metadata": {},
   "source": [
    "**Task 2:** Implement a function `get_all_states()` that returns a list of all possible states in the Optimal Investment Markov Decision Model up to the timepoint `T` in the form of tuples. The structure of the tuple should be `(timestep, asset_price, wealth)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93541d34-f976-4f77-8e69-0d02f032d9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28485\n",
      "Number of calculated states: 737. This number should be equal to 482\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(states))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of calculated states: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(states)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. This number should be equal to 482\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(states) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m482\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_all_states():\n",
    "    # ---- TODO ----\n",
    "    # more like all one step reachable states s' s.t. s'|s,a, where s is start state and a admissible action !FALSE\n",
    "    # all accessible states in the entire MDP, i.e. \"Image\" of S basically, which is dependent from R and a\n",
    "    # hence also the states later in time\n",
    "    \n",
    "    # loop over every attainable state, admissible controls, asset price change\n",
    "    # every admissible control lead to several new states depending on asset price change\n",
    "    \n",
    "    all_states = [start_state]\n",
    "    for state in all_states:\n",
    "        if state[0] < T:\n",
    "            for a in get_admissible_actions(state[1], state[2]):\n",
    "                for R in [u, d]:\n",
    "                    new_state = (state[0]+1, state[1]*(1+R), state[2]+a*R)\n",
    "                    all_states.append(new_state)\n",
    "\n",
    "    return all_states\n",
    "\n",
    "states = get_all_states()\n",
    "print(len(states))\n",
    "# remove duplicate states\n",
    "states = list(set(states))\n",
    "print(f'Number of calculated states: {len(states)}. This number should be equal to 482')\n",
    "assert len(states) == 482"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9c3022",
   "metadata": {},
   "source": [
    "**Task 3:** Implement a function `bellman_optimality_equation(states)` that takes as input the list of all possible states and returns a dictionary, where the keys are all possible states (in form of tuples), and the value for each key is a dictionary containing the optimal action and the value of that state (with string keys `\"opt_a\"` and `\"V\"`) computed with the Bellman optimality equation given in Exercise 1.\n",
    "\n",
    "*Hints:*\n",
    "1. Start by computing the value function of all terminal wealths (the optimal action can be set to `None`).\n",
    "2. Working backwards in time, starting at time `t = T-1`, iterate over all states with timepoint `t`, then iterate over all admissible actions and compute the value following that action. The optimal action is the action with the highest expected value in the next state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08dd6a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_optimality_equation(states):\n",
    "    # ---- TODO ----\n",
    "    \n",
    "    # Initiate necessary nested dictionary list with all information and correct terminal value\n",
    "    bellman_dict = {}    \n",
    "    for state in states:\n",
    "        if state[0] == T:\n",
    "            bellman_dict[state] = {\"opt_a\" : None, \"V\" : np.log(state[2])}\n",
    "    # all dictionary keys are unique!!\n",
    "    \n",
    "    # Value function backwards, by selecting for every state the best action for all potential outcomes\n",
    "    # max[admissible actions driving sum consisting of two terms]\n",
    "    for t in range(T-1, -1, -1):\n",
    "\n",
    "        # fix state s*\n",
    "        for state in states:\n",
    "\n",
    "            if state[0] == t:\n",
    "\n",
    "                # get admissible actions on s*\n",
    "                admissible_actions = get_admissible_actions(state[1], state[2])\n",
    "                \n",
    "                # determine for s* and action the two achievable states s1, s2 and sum\n",
    "                value_t_a = [(q*bellman_dict[(state[0]+1, state[1]*(1+u), state[2]+a*u)][\"V\"]\n",
    "                            +(1-q)*bellman_dict[(state[0]+1, state[1]*(1+d), state[2]+a*d)][\"V\"])\n",
    "                            for a in admissible_actions]\n",
    "                \n",
    "                # get optimizig parameters\n",
    "                V = max(value_t_a)\n",
    "                opt_a = admissible_actions[value_t_a.index(V)]\n",
    "                \n",
    "                bellman_dict[state] = {\"opt_a\": opt_a, \"V\": V}        \n",
    "    \n",
    "    \n",
    "    # return nested dictionary of the form \n",
    "    # {(0,8,100): {\"opt_a\": x, \"V\":y }, (t,p,w): {\"opt_a\": x, \"V\":y }, ...}\n",
    "    \n",
    "    return bellman_dict\n",
    "\n",
    "\n",
    "\n",
    "bellman_dict = bellman_optimality_equation(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba5460f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bellman_optimality_equation_Me(states):\n",
    "    # ---- TODO ----\n",
    "    \n",
    "    # Initiate necessary nested dictionary list with all information and correct terminal value\n",
    "    bellman_dict = {state: {\"opt_a\": None, \"V\": np.log(state[2])} if state[0] == T else {\"opt_a\": None, \"V\": 0}\n",
    "                    for state in states}\n",
    "    # all dictionary keys are unique!!\n",
    "    \n",
    "    # Value function backwards, by selecting for every state the best action for all potential outcomes\n",
    "    # max[admissible actions driving sum consisting of two terms]\n",
    "    for t in range(T-1, -1, -1):\n",
    "        \n",
    "        states_t = (state for state in states if state[0] == t)\n",
    "        \n",
    "        # fix state s*\n",
    "        for state_t in states_t:\n",
    "            \n",
    "            admissible_actions = get_admissible_actions(state_t[1], state_t[2])\n",
    "            \n",
    "            \n",
    "            # get admissible actions on s*\n",
    "            state_t_list_a = []\n",
    "            for a in admissible_actions:\n",
    "\n",
    "                # determine for s* and action the two achievable states s1, s2 and sum\n",
    "                state_t_a = (q*bellman_dict[(state_t[0]+1, state_t[1]*(1+u), state_t[2]+a*u)][\"V\"]\n",
    "                            +(1-q)*bellman_dict[(state_t[0]+1, state_t[1]*(1+d), state_t[2]+a*d)][\"V\"])\n",
    "                \n",
    "                state_t_list_a.append(state_t_a)\n",
    "            \n",
    "            V = max(state_t_list_a)\n",
    "            opt_a = admissible_actions[state_t_list_a.index(V)]\n",
    "            \n",
    "            bellman_dict[state_t][\"V\"] = V # FORGOT TO ADD THIS... TOOK ME AN HOUR TO WORK AROUND W/ SOLUTION\n",
    "            bellman_dict[state_t][\"opt_a\"] = opt_a        \n",
    "    \n",
    "    \n",
    "    \n",
    "    # return nested dictionary of the form \n",
    "    # {(0,8,100): {\"opt_a\": x, \"V\":y }, (t,p,w): {\"opt_a\": x, \"V\":y }, ...}\n",
    "    \n",
    "    return bellman_dict\n",
    "\n",
    "\n",
    "\n",
    "bellman_dict = bellman_optimality_equation_Me(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e7c4074",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.2738410068014625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bellman_dict[start_state]['V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7ee13ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Test function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39misclose(bellman_dict[start_state][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;241m4.781250991199082\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m bellman_dict[start_state][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt_a\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m48.0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Test function\n",
    "assert np.isclose(bellman_dict[start_state]['V'], 4.781250991199082)\n",
    "assert bellman_dict[start_state]['opt_a'] == 48.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53023c0",
   "metadata": {},
   "source": [
    "**Task 4:** Visualise the optimal strategy. To do this, create a Matplotlib scatter plot with the current wealth on the x-axis and the amount of  money invested in the financial asset on the y-axis. First, plot all possible wealth action pairs. Next, highlight the optimal wealth action pairs. \n",
    "\n",
    "Try to reproduce Figure 1.5 from the script. The formula for the black line in Figure 1.5 is given by\n",
    "    $$\\mathrm{black line}(w) = \\left(-\\frac{q}{d}-\\frac{1-q}{u}\\right) \\cdot w.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9984747a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(2.0, 12.0, 256.0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m T:\n\u001b[1;32m     10\u001b[0m     plot_states\u001b[38;5;241m.\u001b[39mappend((state[\u001b[38;5;241m2\u001b[39m],action))\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[43mbellman_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopt_a\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m     12\u001b[0m         plot_states_opt\u001b[38;5;241m.\u001b[39mappend((state[\u001b[38;5;241m2\u001b[39m],action))\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m min_wealth \u001b[38;5;241m>\u001b[39m state[\u001b[38;5;241m2\u001b[39m]:\n",
      "\u001b[0;31mKeyError\u001b[0m: (2.0, 12.0, 256.0)"
     ]
    }
   ],
   "source": [
    "# wealth at all time points and possible actions\n",
    "\n",
    "plot_states = []\n",
    "plot_states_opt = []\n",
    "min_wealth = + np.inf\n",
    "max_wealth = - np.inf\n",
    "for state in states:\n",
    "    for action in get_admissible_actions(p = state[1], w = state[2]):\n",
    "        if state[0] < T:\n",
    "            plot_states.append((state[2],action))\n",
    "            if action == bellman_dict[state][\"opt_a\"]:\n",
    "                plot_states_opt.append((state[2],action))\n",
    "            if min_wealth > state[2]:\n",
    "                min_wealth = state[2]\n",
    "            if max_wealth < state[2]:\n",
    "                max_wealth = state[2]\n",
    "\n",
    "# plot the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(*zip(*plot_states), c='#0065bd')\n",
    "ax.scatter(*zip(*plot_states_opt), c='#F7811E')\n",
    "opt_frac = -q/d-(1-q)/u\n",
    "ax.plot([min_wealth, max_wealth], [opt_frac*min_wealth, opt_frac*max_wealth], c='black')\n",
    "plt.xlabel(\"Total Wealth\")\n",
    "plt.ylabel(\"Wealth in Financial Asset\")\n",
    "plt.savefig(\"opt_policy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45824cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
